{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarlosMendez1997Sei/WETSAT_v2/blob/main/2_Modelling_WETSAT_Google_Colab/Wetsat_Geoprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t\t\t\t\t\t                           Google Colaboratory\n",
        "\t\t\t\t\t\t                              PostgreSQL\n",
        "\t\t\t\t\t\t                                GitHub\t     \n",
        "\t\t\t\t\t\tWetlands flooding extent and trends using SATellite data and Machine Learning WETSAT\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tCode Developed by\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t Carlos Mendez\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  Sebastian Palomino\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tCamilo Sanabria                     "
      ],
      "metadata": {
        "id": "2ZbneqAeST4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages and libraries used in WETSAT"
      ],
      "metadata": {
        "id": "GqLbuH9NMZHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "###################################### Artificial Intelligence Frameworks #####################################################\n",
        "# Geospatial Artificial Intelligence (GeoAI)\n",
        "%pip install geoai-py\n",
        "# scikit-learn Framework\n",
        "!pip install scikit-learn\n",
        "# Tensorflow Framework\n",
        "!pip install tensorflow\n",
        "# Keras Framework\n",
        "!pip install Keras\n",
        "# PyTorch Framework\n",
        "!pip torch torchvision\n",
        "###################################### Data, Geoprocessing and Graphics libraries #####################################################\n",
        "!pip install rasterio\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "f86bUNoSh3wy",
        "outputId": "ecdc0a07-8612-479b-8017-5d9c8570e61a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n###################################### Artificial Intelligence Frameworks #####################################################\\n# Geospatial Artificial Intelligence (GeoAI)\\n%pip install geoai-py\\n# scikit-learn Framework\\n!pip install scikit-learn\\n# Tensorflow Framework\\n!pip install tensorflow\\n# Keras Framework\\n!pip install Keras\\n# PyTorch Framework\\n!pip torch torchvision\\n###################################### Data, Geoprocessing and Graphics libraries #####################################################\\n!pip install rasterio\\n!pip install matplotlib\\n!pip install numpy\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and packages"
      ],
      "metadata": {
        "id": "voMEhHdQMfSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geoai\n",
        "import tensorflow\n",
        "import keras\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "kkKoT8ZyJEO4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone Repository from WETSAT [GitHub](https://github.com/sei-latam/WETSAT_v2)"
      ],
      "metadata": {
        "id": "JweosQFkOUfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sei-latam/WETSAT_v2.git"
      ],
      "metadata": {
        "id": "aFkH-wZjHbhM",
        "outputId": "46c1662d-7fa1-4b94-88d7-ea3cef5151ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WETSAT_v2'...\n",
            "remote: Enumerating objects: 585, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 585 (delta 11), reused 7 (delta 5), pack-reused 569 (from 3)\u001b[K\n",
            "Receiving objects: 100% (585/585), 1.41 GiB | 20.92 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "Updating files: 100% (246/246), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Sentinel-1 Raster Images (VH-VV polarization)"
      ],
      "metadata": {
        "id": "gherFqUmvcmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vh_path = \"/content/WETSAT_v2/0_Original_Files/aoi1/aoi1/gamma_dB/VH_gamma_dB/\"\n",
        "vv_path = \"/content/WETSAT_v2/0_Original_Files/aoi1/aoi1/gamma_dB/VV_gamma_dB/\"\n",
        "\n",
        "# Create a function to load images in a folder\n",
        "def load_images(folder_path): # Create a request 'folder path' as a directory of data\n",
        "    images = [] # Create a temporal empty array of images\n",
        "    for file in sorted(os.listdir(folder_path)): # Call the folfer 'path'\n",
        "        if file.endswith(\".tif\"): # Search images ending in .tif\n",
        "            with rasterio.open(os.path.join(folder_path, file)) as src: # Open the images located in folder path\n",
        "                images.append(src.read(1))\n",
        "    return np.array(images) # Return array of images\n",
        "\n",
        "# Call function and load images\n",
        "vh_images = load_images(vh_path)\n",
        "vv_images = load_images(vv_path)\n",
        "\n",
        "print(f\"System recognized {vh_images.shape[0]} images VH polarization with size {vh_images.shape[1:]} each one.\")\n",
        "print(f\"System recognized {vv_images.shape[0]} images VV polarization with size {vv_images.shape[1:]} each one.\")\n",
        "assert vh_images.shape == vv_images.shape, \"The dimensions and size of VH-VV polarizations are equal\"\n",
        "\n",
        "# Combine the VH-VV polarization with the same size and dimensions\n",
        "X = np.stack([vh_images, vv_images], axis=-1)  # (n_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5XcVxJKZlS",
        "outputId": "5cfb3c5c-3d14-4988-d9bb-db007404c88a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System recognized 29 images VH polarization with size (1121, 967) each one.\n",
            "System recognized 29 images VV polarization with size (1121, 967) each one.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import points (.shp) with labels and marks"
      ],
      "metadata": {
        "id": "GZlztOSGvmbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open shapefile folder\n",
        "shapefile_path = \"/content/WETSAT_v2/0_Original_Files/aoi1/aoi1/points_AOI1_Mask/points_AOI1.shp\"\n",
        "# Import geopandas and read .shp\n",
        "gdf = gpd.read_file(shapefile_path)\n",
        "# Get coordinates of .shp\n",
        "coords = [(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry]\n",
        "# Get values of the column \"gridcode\"\n",
        "labels = gdf[\"gridcode\"].values\n",
        "# Print values\n",
        "print(f\"The marks and labels found in .shp are: {labels}\")"
      ],
      "metadata": {
        "id": "qf5EK-_Kkxax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4093edfa-acd7-43ec-dbbd-27276fb82202"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The marks and labels found in .shp are: [26 22 26 22 22 24 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create marks and labels using rasterize (convert vector to raster)"
      ],
      "metadata": {
        "id": "GSJKEbK22AYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification of System Reference Coordinates (.SRC) between VH-VV\n",
        "with rasterio.open(os.path.join(vh_path, os.listdir(vh_path)[0])) as src:\n",
        "    transform = src.transform\n",
        "    raster_crs = src.crs\n",
        "\n",
        "# Verify the .SRC with of rasters with the .shp named \"gdf\"\n",
        "gdf = gdf.to_crs(raster_crs)\n",
        "\n",
        "# Create a list between geometry and labels\n",
        "shapes = [(geom, label) for geom, label in zip(gdf.geometry, gdf[\"gridcode\"])]\n",
        "\n",
        "# Rasterize the labels\n",
        "raster_labels = rasterize(\n",
        "    shapes,\n",
        "    out_shape=(X.shape[1], X.shape[2]),  # height and width\n",
        "    transform=transform,\n",
        "    fill=0,\n",
        "    dtype=\"int32\"\n",
        ")\n",
        "\n",
        "print(\"Rasterization complete with Shape:\", raster_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI71of321Rn5",
        "outputId": "46ec8f78-997e-4173-a472-4b452c975d2f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rasterization complete with Shape: (1121, 967)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    img = X[i]  # (alto, ancho, 2)\n",
        "    img_flat = img.reshape(-1, 2)\n",
        "    labels_flat = raster_labels.flatten()\n",
        "    mask = labels_flat > 0\n",
        "\n",
        "    X_train.append(img_flat[mask])\n",
        "    y_train.append(labels_flat[mask])\n",
        "\n",
        "# Concatenar todos los datos\n",
        "X_train = np.concatenate(X_train, axis=0)\n",
        "y_train = np.concatenate(y_train, axis=0)\n",
        "\n",
        "print(f\"Total muestras: {X_train.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksfYh67Juk2O",
        "outputId": "fe1fb236-c07b-42ff-a0b3-f3106e724b5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total muestras: 203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_tr, y_tr)\n",
        "print(\"Accuracy RF:\", rf.score(X_te, y_te))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSmaqJgQ7eKC",
        "outputId": "9b58be16-6c55-4ce5-def4-9fa2790a84a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy RF: 0.7073170731707317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model_tf = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(2,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(set(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_tf.fit(X_tr, y_tr, epochs=10, validation_data=(X_te, y_te))"
      ],
      "metadata": {
        "id": "o4osk_R47eMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, len(set(y_train)))\n",
        "        )\n",
        "\n",
        "model_pt = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pt.parameters(), lr=0.001)\n",
        "\n",
        "X_tr_tensor = torch.tensor(X_tr, dtype=torch.float32)\n",
        "y_tr_tensor = torch.tensor(y_tr, dtype=torch.long)\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_pt(X_tr_tensor)\n",
        "    loss = criterion(outputs, y_tr_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "sj9azhwL7s-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmCu3P6M7tBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFvT6Ns47tDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkh79u9C7ePJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ORHEwWVm7eRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "deseo desarrollar un programa y código en Google Colab que utilice los frameworks Tensorflow, Keras, scikit-learn y Pytorch, para leer imágenes Sentinel-1 de un github, con carpetas separadas (VH y VV), luego utilizar las diferentes etiquetas de datos proveniente de un archivo de puntos shapefile (alojado en GitHub). Finalmente, usar los modelos para pronosticar o identificar posibles etiquetas en las imágenes S1."
      ],
      "metadata": {
        "id": "EfGJ3AgmPr5c"
      }
    }
  ]
}